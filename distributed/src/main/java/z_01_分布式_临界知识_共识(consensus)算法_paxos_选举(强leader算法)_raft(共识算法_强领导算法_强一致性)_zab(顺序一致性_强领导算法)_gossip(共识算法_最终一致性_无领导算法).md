#临界知识
延时限制
减少准备阶段优化rpc过程
Multi-Paxos,主备、强领导者模型的共识算法
主节点宕机选举leader
#Basic Paxos(选举leader)
```asp
其实为了提高可用性、可靠性等，总是会有多个副本，meta server 有多个副本，data 也有多个副本，多个副本虽然能提高可靠性和可用性，但是也带来了新的问题，
那就是需要保证多个副本之间的共识，也就是多个副本对任何一个提案（或者说 Op）是一致的，一个 Op 如何来达到共识，就像 4 个人提案去哪里吃饭，需要保证一些基本的特性：

```
![](.z_01_分布式_临界知识_共识算法_一致性算法_raft_paxos(选举)_gossip_images/c0d50017.png)
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_raft_paxos(选举)_gossip_images/f99e4803.png)
##角色
###提议者
提议一个值，用于投票表决，发起二阶段提交，进行共识协商;
###接受者
对每个提议的值进行投票，并存储接受的值，比如 A、B、C 三 个节点。 一般来说，集群中的所有节点都在扮演接受者的角色，参与共识协商，并接受 和存储数据
一个节点(或进程)可以身兼多个角色，对提议的值进行投票，并接受达成共识的值，存储保存;
![](.z_01_分布式_临界知识_共识算法_一致性算法_raft_paxos(选举)_gossip_images/3221564d.png)
###学习者
####学习者作用?
```asp
被告知投票的结果，接受达成共识的值，存储保存，不参与投票的 过程。一般来说，学习者是数据备份节点，比如“Master-Slave”模型中的 Slave，
被动地接受数据，容灾备份。
```
##达成共识过程
###涉及对象
角色  
提案:提案号，提案值,

状态:达成共识
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_raft_paxos(选举)_gossip_images/b23ea036.png)

###提案二阶段
```asp
如果准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者将承诺不响应这个准备请求；

如果接受请求中的提案的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者将承诺不通过这个提案；

如果接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中，包含已经通过的最大编号的提案信息。
```
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_raft_paxos(选举)_gossip_images/427a67bb.png)
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_raft_paxos(选举)_gossip_images/4d6fe5ef.png)
####准备(Prepare)阶段
谁的提案号更大，接受者就使用谁的提案号,存在多个提议者
只有准备阶段，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段(也就是接受阶段
![](.z_01_分布式_临界知识_共识算法_一致性算法_raft_paxos(选举)_gossip_images/9fcb9866.png)
![](.z_01_分布式_临界知识_共识算法_一致性算法_raft_paxos(选举)_gossip_images/ff7f9565.png)
####接受阶段
此过程存在多个通过准备节点的提议者，但只有n/2 +1接受者同意,则通过提案，
![](.z_01_分布式_临界知识_共识算法_一致性算法_raft_paxos(选举)_gossip_images/341a5423.png)
###问题
```asp
在示例中，如果节点 A、B 已经通过了提案[5, 7]，节点 C 未通过任何提案，那么当客户端 3 提案编号为 9 时，通过 Basic Paxos 执行“SET X = 6”，最终三个节点上 X 值是多少呢？


大多数节点就某个值，达成共识后，值就不再变了，哪怕有新的提案，这个值，也能保证不再变了。”这个最后的值肯定是7，至于编号嘛，取最大的
```
##可用性
```asp
Basic Paxos 还实现了容错，在少于一半的节点出现故障时，集群也能工作。 它不像分布式事务算法那样，必须要所有节点都同意后才提交操作，
因为“所有节点都 同意”这个原则，在出现节点故障的时候会导致整个集群不可用。也就是说，“大多数 节点都同意”的原则，赋予了 Basic Paxos 容错的能力，让它能够容忍少于一半的节点 的故障。

```

#Multi-Paxos
##选举leader
只有领导 者节点一个提议者,节省了准备阶段，存在多个提议者同时提交提案的情况，也就不存在提 案冲突的情况了。
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_raft_paxos(选举)_gossip_images/6943b29e.png)
##强一致性
将数据发送给所有的节点，并且在大多数的服务器接受 了这个写请求之后，再响应给客户端成功:
容错(n - 1)/2 个节点的 故障。
##问题

#raft(共识对象:leader,应用与leader选举)
[可视化](http://thesecretlivesofdata.com/raft/)
从本质上说，Raft 算法是通过一切以 领导者为准的方式，实现一系列值的共识和各节点日志的一致
只能有一个leader，是优化版的multi-paxos
##raft是cp还是ap?
强一致性算法,强一致性强调的是强一致性读，既不会读到过期的结果
Raft是符合这个强一致性定义的，通过每次读取前都先由leader确认无落后的Committed entries，Raft保证任何时候都不会返回过期的数据
Raft handles this by having the leader exchange heartbeat messages with a majority of the cluster before responding to read-only requests.
[](https://lentil1016.cn/consistencies-and-raft/)
其实 Raft 不是一致性算法而是共识算法，是一个 Multi-Paxos 算法，实现的是如何就一系列值达成共识。并且，Raft 能容忍少数节点的故 障。
虽然 Raft 算法能实现强一致性，也就是线性一致性(Linearizability)，但需要客户 端协议的配合。
##临界知识
每个节点等待领导者节点心跳信息的超 时时间间隔是随机的
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/cbba631d.png)
总共三种角色
选举任期
领导者心跳消息
随机选举超时时间
先来先服务的投票原则
大多数选票
日志同步
避免脑裂,单节点变更
raft能实现强一致性实现cp
raft是共识算法(multi-paxos),不是一致性算法
##角色
###领导者
一个任期只有一位领导
处理 写请求、管理日志复制和不断地发送心跳信息，通知其他节点“我是领导者，我还活 着，你们现在不要发起新的选举，找个新领导者来替代我。”
###候选人
候选人将向其他节点发送请求投票(RequestVote)RPC 消息，通知其他节点 来投票，如果赢得了大多数选票，就晋升当领导者
###跟随者
就相当于普通群众，默默地接收和处理来自领导者的消息，当等待领导者心跳
 信息超时的时候，就主动站出来，推荐自己当候选人

##节点间通讯
```asp
1. 请求投票(RequestVote)RPC，是由候选人在选举期间发起，通知各节点进行投票;
2. 日志复制(AppendEntries)RPC，是由领导者发起，用来复制日志和提供心跳消息。
我想强调的是，日志复制 RPC 只能由领导者发起，这是实现强领导者模型的关键之一，希 望你能注意这一点，后续能更好地理解日志复制，理解日志的一致是怎么实现的。
```
##leader选举过程
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/59c20d36.png)
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/9963f503.png)
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/e376b3a3.png)
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/a9a01648.png)
###任期
```asp
1. 跟随者在等待领导者心跳信息超时后，推举自己为候选人时，会增加自己的任期号，比 如节点 A 的当前任期编号为 0，那么在推举自己为候选人时，会将自己的任期编号增加 为 1。

2. 如果一个服务器节点，发现自己的任期编号比其他节点小，那么它会更新自己的编号到 较大的编号值。比如节点 B 的任期编号是 0，当收到来自节点 A 的请求投票 RPC 消息 时，
因为消息中包含了节点 A 的任期编号，且编号为 1，那么节点 B 将把自己的任期编 号更新为 1。

1. 在 Raft 算法中约定，如果一个候选人或者领导者，发现自己的任期编号比其他节点小， 那么它会立即恢复成跟随者状态。比如分区错误恢复后，任期编号为 3 的领导者节点 B，
收到来自新领导者的，包含任期编号为 4 的心跳消息，那么节点 B 将立即恢复成跟 随者状态。

2. 还约定如果一个节点接收到一个包含较小的任期编号值的请求，那么它会直接拒绝这个 请求。比如节点 C 的任期编号为 4，收到包含任期编号为 3 的请求投票 RPC 消息，那
么它将拒绝这个消息。
```
###选举规则
[](https://zhuanlan.zhihu.com/p/27207160)
```asp
1. 领导者周期性地向所有跟随者发送心跳消息(即不包含日志项的日志复制 RPC 消息)， 通知大家我是领导者，阻止跟随者发起新的选举。
2. 如果在指定时间内，跟随者没有接收到来自领导者的消息，那么它就认为当前没有领导 者，推举自己为候选人，发起领导者选举。
3. 在一次选举中，赢得大多数选票的候选人，将晋升为领导者。
4. 在一个任期内，领导者一直都会是领导者，直到它自身出现问题(比如宕机)，或者因 为网络延迟，其他节点发起一轮新的选举。
5. 在一次选举中，每一个服务器节点最多会对一个任期编号投出一张选票，并且按照“先 来先服务”的原则进行投票。比如节点 C 的任期编号为 3，
先收到了 1 个包含任期编号 为 4 的投票请求(来自节点 A)，然后又收到了 1 个包含任期编号为 4 的投票请求(来 自节点 B)。那么节点 C 将会把唯一一张选票投给节点 A，
当再收到节点 B 的投票请求 RPC 消息时，对于编号为 4 的任期，已没有选票可投了。
```
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/e9248c41.png)
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/d3f4a98e.png)
###避免选举失败
```asp
如何理解随机超时时间
在议会选举中，常出现未达到指定票数，选举无效，需要重新选举的情况。在 Raft 算法的 选举中，也存在类似的问题，那它是如何处理选举无效的问题呢?
其实，Raft 算法巧妙地使用随机选举超时时间的方法，把超时时间都分散开来，在大多数 情况下只有一个服务器节点先发起选举，而不是同时发起选举，这样就能减少因选票瓜分导 致选举失败的情况。


1. 跟随者等待领导者心跳信息超时的时间间隔，是随机的;
2. 当没有候选人赢得过半票数，选举无效了，这时需要等待一个随机时间间隔，也就是 说，等待选举超时的时间间隔，是随机的。
```
###raft的单leader问题
写落在leader上，写单点,写无法扩容
```asp
raft算法的局限：
1、强领导模型对于写功能基本退化单机性能，量大任然会出现性能瓶颈，适得其反。
2、选举期间会集群将出现短暂不可用现象，影响时长与选举时间相关。
```
##日志同步
###日志格式
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/a0d49ce3.png)
```asp
指令:一条由客户端请求指定的、状态机需要执行的指令。你可以将指令理解成客户端
 指定的数据。
索引值:日志项对应的整数索引值。它其实就是用来标识日志项的，是一个连续的、单
 调递增的整数号码。
任期编号:创建这条日志项的领导者的任期编号。
```
###日志一致性
```asp
首先，领导者进入第一阶段，通过日志复制(AppendEntries)RPC 消息，将日志项复制 到集群其他节点上。
接着，如果领导者接收到大多数的“复制成功”响应后，它将日志项提交到它的状态机，并
返回成功给客户端。如果领导者没有接收到大多数的“复制成功”响应，那么就返回错误给
客户端。
学到这里，有同学可能有这样的疑问了，领导者将日志项提交到它的状态机，怎么没通知跟
随者提交日志项呢?
这是 Raft 中的一个优化，领导者不直接发送消息通知其他节点提交指定日志项。因为领导 者的日志复制 RPC 消息或心跳消息，包含了当前最大的，
将会被提交的日志项索引值。所 以通过日志复制 RPC 消息或心跳消息，跟随者就可以知道领导者的日志提交位置信息。
因此，当其他节点接受领导者的心跳消息，或者新的日志复制 RPC 消息后，就会将这条日 志项提交到它的状态机。而这个优化，
降低了处理客户端请求的延迟，将二阶段提交优化为 了一段提交，降低了一半的消息延迟。
```
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/b29859d0.png)
```asp
1. 接收到客户端请求后，领导者基于客户端请求中的指令，创建一个新日志项，并附加到 本地日志中。
2. 领导者通过日志复制 RPC，将新的日志项复制到其他的服务器。
3. 当领导者将日志项，成功复制到大多数的服务器上的时候，领导者会将这条日志项提交 到它的状态机中。
4. 领导者将执行的结果返回给客户端。
5. 当跟随者接收到心跳信息，或者新的日志复制 RPC 消息后，如果跟随者发现领导者已经 提交了某条日志项，而它还没提交，那么跟随者就将这条日志项提交到本地的状态机 中。
```
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/e472b0d3.png)
```asp
1. 领导者通过日志复制 RPC 消息，发送当前最新日志项到跟随者(为了演示方便，假设当 前需要复制的日志项是最新的)，这个消息的 PrevLogEntry 值为 7，PrevLogTerm 值 为 4。
2. 如果跟随者在它的日志中，找不到与 PrevLogEntry 值为 7、PrevLogTerm 值为 4 的日 志项，也就是说它的日志和领导者的不一致了，那么跟随者就会拒绝接收新的日志项， 并返回失败信息给领导者。
3. 这时，领导者会递减要复制的日志项的索引值，并发送新的日志项到跟随者，这个消息 的 PrevLogEntry 值为 6，PrevLogTerm 值为 3。
4. 如果跟随者在它的日志中，找到了 PrevLogEntry 值为 6、PrevLogTerm 值为 3 的日志 项，那么日志复制 RPC 返回成功，这样一来，领导者就知道在 PrevLogEntry 值为 6、 PrevLogTerm 值为 3 的位置，跟随者的日志项与自己相同。
5. 领导者通过日志复制 RPC，复制并更新覆盖该索引值之后的日志项(也就是不一致的日 志项)，最终实现了集群各节点日志的一致。
```
日志不仅是数据的载体，日志的完整性还影响领导者选举的结果。也就是说， 日志完整性最高的节点才能当选领导者
Raft 是通过以领导者的日志为准，来实现日志的一致的

###少数日志未同步
如果节点不再大多数节点，只要等心跳来更新自己的任期，还有等到leader 的一致性检查来修复自己的日志
##节点增删/成员变更
如果遇到需要改变数据副本数的情况，则需要增加或移除集群中的服务器。
对集群成员进行变更时(比如 台服务器)，因为集群分裂，可能出现 2 个领导者
###避免两个领导-单节点变更
[](https://juejin.cn/post/6975512357640339463#heading-5)
节点 A、B 和 C 之间发生了分区错误，节点 A、B 组成旧配置中的“大多 数”，也就是变更前的 3 节点集群中的“大多数”，那么这时的领导者(节点 A)依旧是 领导者。  
另一方面，节点 C 和新节点 D、E 组成了新配置的“大多数”，也就是变更后的 5 节点集 群中的“大多数”，它们可能会选举出新的领导者(比如节点 C)。那么这时，就出现了同 时存在 2 个领导者的情况
![](.z_01_分布式_临界知识_共识(consensus)算法_一致性算法_paxos(选举)_raft_gossip_images/66298814.png)
就是通过一次变更一个节点实现成员变更。如果需要变更多个节点，那你需要 执行多次单节点变更。比如将 3 节点集群扩容为 5 节点集群，
这时你需要执行 2 次单节点 变更，先将 3 节点集群变更为 4 节点集群，然后再将 4 节点集群变更为 5 节点集
###单节点变更的问题
可以在领导者启动时，创建一个 NO_OP 日志项(也就是空日志 项)，只有当领导者将 NO_OP 日志项提交后，再执行成员变更请求。这个解决办法，
你 记住就可以了，可以自己在课后试着研究下
##使用场景
配置中心、名字服务以及时序数 据库的 META 节点，采用Raft 算法
设计时序数据库的 DATA 节点一致性时，基于 水平扩展、性能和数据完整性等考虑，就没采用 Raft 算法，而是采用了 Quorum NWR、 失败重传、反熵等机制
##强领导者模型会限制集群的写性能，那你想想看，有什么办法能突破 Raft 集群的写性能瓶颈呢？
可以参考Kafka的分区和ES的主分片副本分片这种机制，虽然写入只能通过leader写，但每个leader可以负责不同的片区，来提高写入的性能
##那么在多个 Raft 集群组成的 KV 系统中，如何设计一致哈希，实现当某个集群的领导者节点出现故障，并选举出新的领导者后，整个系统还能稳定运行呢
相比“值到节点”的一级映射，可以做两级映射，“值到集群，集群到领导者节点”，通过Raft的节点故障容错能力，来避免数据迁移


#gossip

#zab
[z_03_分布式_服务注册中心_02_zookeeper_01_ZAB共识算法_过半机制_初始选举_崩溃恢复_广播消息.md]
##选举
##故障恢复

##zab和raft的区别
raft算法跟zab的选举区别，可以理解为比较大的区别就是zab是有leader PK，而raft只是先来先得，一旦该节点已经确认投票，后面即使比他任期编号大的选票再来请求投票，也会拒绝
Raft领导者选举的关键是随机超时时间、一个节点在一个任期只有一张选票、基于任期编号大小和日志完整度来投票
[](https://time.geekbang.org/column/article/237950)
