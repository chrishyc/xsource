##临界知识
分片集群扩展时的热点问题
数据倾斜问题
虚拟分区节点
##hash算法
```
使用特定的数据，如Redis的键或用户ID，再根据节点数量N使用公式: hash(key)%N计算出哈希值，用来决定数据映射到哪一个节点上。
这种方 案存在一个问题:当节点数量变化时，如扩容或收缩节点，数据节点映射关 系需要重新计算，会导致数据的重新迁移。
这种方式的突出优点是简单性，常用于数据库的分库分表规则，一般采 用预分区的方式，提前根据数据量规划好分区数，
比如划分为512或1024张 表，保证可支撑未来一段时间的数据量，再根据负载情况将表迁移到其他数 据库中。
扩容时通常采用翻倍扩容，避免数据映射全部被打乱导致全量迁移 的情况
```
![](.z_01_分布式_临界知识_分区算法_hash算法_redis一致hash算法_mysql分区算法_减少数据迁移_images/7c205f6c.png)
##分片集群热点问题
##一致hash算法
假设 key-01、key-02、 key-03 三个 key，经过哈希算法 c-hash() 计算后，在哈希环上的位置就像图 6 的样子
![](.z_01_分布式_临界知识_一致hash算法_images/7d06200a.png)
```asp
首先，将 key 作为参数执行 c-hash() 计算哈希值，并确定此 key 在环上的位置; 
然后，从这个位置沿着哈希环顺时针“行走”，遇到的第一节点就是 key 对应的节点
```
```asp
你可以看到，key-01 和 key-02 不会受到影响，只有 key-03 的寻址被重定位到 A。一般 来说，在一致哈希算法中，如果某个节点宕机不可用了，
那么受影响的数据仅仅是，会寻址 到此节点和前一节点之间的数据。比如当节点 C 宕机了，受影响的数据是会寻址到节点 B 和节点 C 之间的数据(例如 key-03)，寻址到其他哈希环空间的数据(例如 key-01)， 不会受到影响。
```
![](.z_01_分布式_临界知识_一致hash算法_images/f706dbc8.png)
![](.z_01_分布式_临界知识_一致hash算法_images/f30cc63d.png)
对于 1000 万 key 的 3 节点 KV 存储，如 果我们增加 1 个节点，变为 4 节点集群，只需要迁移 24.3% 的数据:
总的来说，使用了一致哈希算法后，扩容或缩容的时候，都只需要重定位环空间中的一小部 分数据。也就是说，一致哈希算法具有较好的容错性和可扩展性。
##数据倾斜问题
客户端访问请求集中在少数的节点上， 出现了有些机器高负载，有些机器低负载的情况，那么在一致哈希中，有什么办法能让数据 访问分布的比较均匀呢?答案就是虚拟节点。
在一致哈希中，如果节点太少，容易因为节点分布不均匀造成数据访问的冷热不均，也就是
说大多数访问请求都会集中少量几个节点上
![](.z_01_分布式_临界知识_一致hash算法_images/96ec4165.png)

其实，就是对每一个服务器节点计算多个哈希值，在每个计算结果位置上，都放置一个虚拟 节点，并将虚拟节点映射到实际节点。比如，可以在主机名的后面增加编号，
分别计算 “Node-A-01”“Node-A-02”“Node-B-01”“Node-B-02”“Node-C- 01”“Node-C-02”的哈希值，于是形成 6 个虚拟节点:
![](.z_01_分布式_临界知识_一致hash算法_images/22bd8912.png)
如果使用了哈希 算法，需要迁移高达 90.91% 的数据，使用一致哈希的话，只需要迁移 6.48% 的数据。
我希望你能注意到这个规律，使用一致哈希实现哈希寻址时，可以通过增加节点数降低节点
宕机对整个集群的影响，以及故障恢复时需要迁移的数据量。后续在需要时，你可以通过增
加节点数来提升系统的容灾能力和故障恢复效率。
，一致哈希本质上是一种路由寻址算法，适合简单的路由寻址场景。比如在 KV 存储系统内部，它的特点是简单，不需要维护路由信息

##redis虚拟分区slot
![](.z_01_分布式_临界知识_分区算法_hash算法_redis一致hash算法_mysql分区算法_减少数据迁移_images/bac586d1.png)
![](.z_01_分布式_临界知识_分区算法_hash算法_redis一致hash算法_mysql分区算法_减少数据迁移_images/30241659.png)
###redis虚拟分区为啥能减少数据迁移?
##多集群故障
那么在多个 Raft 集群组成的 KV 系统中，如何设计一致哈希，实现当某个集群的领导者节点出现故障，并选举出新的领导者后，整个系统还能稳定运行呢？


